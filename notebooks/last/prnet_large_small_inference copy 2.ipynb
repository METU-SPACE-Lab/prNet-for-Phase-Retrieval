{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from dataloaders.dataloader_v1 import get_loader\n",
    "import torch\n",
    "from wcmatch.pathlib import Path\n",
    "from utils.utils import crop_center_half, ifft2d, normalize, flip_to_minimize_loss\n",
    "from utils.algorithms import get_algorithm\n",
    "from matplotlib import pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torchmetrics.functional.image import structural_similarity_index_measure, peak_signal_noise_ratio\n",
    "\n",
    "root = \"/hdd_mnt/onurcan/onurk/datasets/\"\n",
    "stage = \"test\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.utils import zero_padding_twice, fft2d, ifft2d\n",
    "from utils.algorithms import apply_image_constraint_hio\n",
    "from models.denoisers import get_denoiser\n",
    "\n",
    "class End2End(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        total_iterations: int = 12,\n",
    "        total_dc_iterations: int = 5,\n",
    "        denoiser_architecture: str = \"FBPConvNet\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.denoiser = get_denoiser(denoiser_architecture)().to(device)\n",
    "\n",
    "        self.lam = torch.nn.Parameter(\n",
    "            torch.logspace(-0.11, -1.9, total_iterations).to(device)\n",
    "        )  # 1....0.125 from the thesis, lam should be [0,1]\n",
    "        \n",
    "        self.total_iterations = total_iterations\n",
    "        self.total_dc_iterations = total_dc_iterations\n",
    "        self.beta = 0.9\n",
    "        self.alpha = 3.0\n",
    "\n",
    "    def forward(self, hio_output, amplitude, support, total_time_steps = None, last_iterations_to_train = 1):\n",
    "        total_time_steps = total_time_steps or self.total_iterations\n",
    "\n",
    "        assert total_time_steps <= self.total_iterations, \"total_time_steps should be smaller than total_iterations\"\n",
    "        assert last_iterations_to_train <= total_time_steps, \"last_iterations_to_train should be smaller than total_time_steps\"\n",
    "            \n",
    "        x_ = hio_output # B, 10, W, H\n",
    "        \n",
    "        self.denoiser.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(total_time_steps - last_iterations_to_train):\n",
    "                z_ = self.denoiser(x_ / 255.0, i) * 255.0 # B, 5, W, H\n",
    "                z_ = torch.clamp(z_, min=0.0, max=255.0)\n",
    "                \n",
    "                x_ = self.dc(z_, amplitude, i, support)\n",
    "                \n",
    "                if (i == total_time_steps - 1) and (last_iterations_to_train == 0):\n",
    "                    return [z_]\n",
    "                \n",
    "                x_ = torch.cat([x_, z_], dim=1)\n",
    "                \n",
    "                x_ = x_ + self.lam[i] * torch.randn_like(x_) * self.alpha * self.alpha\n",
    "\n",
    "        self.denoiser.train()\n",
    "        output_list = []\n",
    "        for i in range(total_time_steps - last_iterations_to_train, total_time_steps):\n",
    "            z_ = self.denoiser(x_ / 255.0, i) * 255.0\n",
    "            z_ = torch.clamp(z_, min=0.0, max=255.0)\n",
    "            \n",
    "            output_list.append(z_) # normalize(x_)\n",
    "            \n",
    "            x_ = self.dc(z_, amplitude, i, support)\n",
    "            \n",
    "            output_list.append(x_) # normalize(x_)\n",
    "            \n",
    "            x_ = torch.cat([x_, z_], dim=1) # before z_ x_\n",
    "            \n",
    "            x_ = x_ + self.lam[i] * torch.randn_like(x_) * self.alpha * self.alpha\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    def dc(self, z, b, i, support):\n",
    "        z_k = zero_padding_twice(z)\n",
    "        z_0 = z_k\n",
    "        \n",
    "        y_new = self.lam[i] * b + (1 - self.lam[i]) * fft2d(z_0).abs()\n",
    "\n",
    "        for _ in range(self.total_dc_iterations):\n",
    "            Fz = fft2d(z_k)\n",
    "            # x_kprime = ifft2d(torch.polar(self.lam[i] * b + (1 - self.lam[i]) * Fz.abs(), Fz.angle())).real\n",
    "            # x_kprime = self.lam[i] * ifft2d(torch.polar(b, Fz.angle())).real + (1 - self.lam[i]) * z_0\n",
    "            x_kprime = ifft2d(torch.polar(y_new, Fz.angle())).real\n",
    "            z_k = apply_image_constraint_hio(x_kprime, z_k, support, beta=self.beta)\n",
    "\n",
    "        return crop_center_half(z_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = 18\n",
    "denoiser_architecture = \"UNet2DMulti\"\n",
    "\n",
    "end2end_model = End2End(device=device, total_iterations=total_iterations, denoiser_architecture=denoiser_architecture)\n",
    "adversarial_denoiser = get_denoiser(\"UNet2DMultiLargeAdversarialNoFreq\")().to(device)\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.utils import zero_padding_twice, fft2d, ifft2d\n",
    "from utils.algorithms import apply_image_constraint_hio\n",
    "from models.denoisers import get_denoiser\n",
    "\n",
    "class End2EndSmall(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        total_iterations: int = 12,\n",
    "        total_dc_iterations: int = 5,\n",
    "        denoiser_architecture: str = \"FBPConvNet\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.denoiser = get_denoiser(denoiser_architecture)().to(device)\n",
    "\n",
    "        self.lam = torch.nn.Parameter(\n",
    "            torch.logspace(-0.15, -2.9, total_iterations).to(device)\n",
    "        )  # 1....0.125 from the thesis, lam should be [0,1]\n",
    "        \n",
    "        self.total_iterations = total_iterations\n",
    "        self.total_dc_iterations = total_dc_iterations\n",
    "        self.beta = 0.9\n",
    "        self.alpha = 3.0\n",
    "\n",
    "    def forward(self, hio_output, amplitude, support, total_time_steps = None, last_iterations_to_train = 1):\n",
    "        total_time_steps = total_time_steps or self.total_iterations\n",
    "\n",
    "        assert total_time_steps <= self.total_iterations, \"total_time_steps should be smaller than total_iterations\"\n",
    "        assert last_iterations_to_train <= total_time_steps, \"last_iterations_to_train should be smaller than total_time_steps\"\n",
    "            \n",
    "        x_ = hio_output\n",
    "        \n",
    "        self.denoiser.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(total_time_steps - last_iterations_to_train):\n",
    "                z_ = self.denoiser(x_ / 255.0, i) * 255.0\n",
    "                z_ = torch.clamp(z_, min=0.0, max=255.0)\n",
    "                x_ = self.dc(z_, amplitude, i, support)\n",
    "                if (i == total_time_steps - 1) and (last_iterations_to_train == 0):\n",
    "                    return [z_, x_]\n",
    "                x_ = x_ + self.lam[i] * torch.randn_like(x_) * self.alpha * self.alpha\n",
    "\n",
    "        self.denoiser.train()\n",
    "        output_list = []\n",
    "        for i in range(total_time_steps - last_iterations_to_train, total_time_steps):\n",
    "            z_ = self.denoiser(x_ / 255.0, i) * 255.0\n",
    "            output_list.append(z_) # normalize(x_)\n",
    "            z_ = torch.clamp(z_, min=0.0, max=255.0)\n",
    "            x_ = self.dc(z_, amplitude, i, support)\n",
    "            output_list.append(x_) # normalize(x_)\n",
    "            x_ = x_ + self.lam[i] * torch.randn_like(x_) * self.alpha * self.alpha\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    def dc(self, z, b, i, support):\n",
    "        z_k = zero_padding_twice(z)\n",
    "        z_0 = z_k\n",
    "\n",
    "        for _ in range(self.total_dc_iterations):\n",
    "            Fz = fft2d(z_k)\n",
    "            # x_kprime = ifft2d(torch.polar(self.lam[i] * b + (1 - self.lam[i]) * Fz.abs(), Fz.angle())).real\n",
    "            x_kprime = ifft2d(torch.polar(self.lam[i] * b, Fz.angle())).real + (1 - self.lam[i]) * z_0\n",
    "            z_k = apply_image_constraint_hio(x_kprime, z_k, support, beta=self.beta)\n",
    "\n",
    "        return crop_center_half(z_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = 18\n",
    "denoiser_architecture = \"UNet2D\"\n",
    "\n",
    "end2end_model_small = End2EndSmall(device=device, total_iterations=total_iterations, denoiser_architecture=denoiser_architecture)\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# large\n",
    "end2end_model.load_state_dict(torch.load(\"save_mololoed_betterscheduler_clamp_morenoise_moreiterations_notzx_new_yesyesyesyesyesyes_allimages_last_amax_newwithcorrectsnr_2__largenew_betterdc.pth\"))\n",
    "adversarial_denoiser.load_state_dict(torch.load(\"save_adversarial_alpha_3_best_large_new_________.pth\"))\n",
    "\n",
    "# small\n",
    "end2end_model_small.load_state_dict(torch.load(\"save_mololoed_betterscheduler_clamp_morenoise_moreiterations_notzx_new_yesyesyesyesyesyes_allimages_best_amax_newwithcorrectsnr.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from utils.utils import crop_center_half, ifft2d, normalize, flip_to_minimize_loss\n",
    "from utils.algorithms import get_algorithm\n",
    "from torchmetrics.functional.image import structural_similarity_index_measure, peak_signal_noise_ratio\n",
    "import logging\n",
    "import pickle\n",
    "from einops import reduce\n",
    "\n",
    "filename = \"notebooks/large_small_inference__alpha4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = get_loader(\"noise_amplitude_dataset\", stage, root, batch_size=1, alpha=4, return_noiseless=True) # dont change the batch size for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(image, save_index, save_index_monte_carlo, save_psnr, save_ssim, save_snr, save_description):\n",
    "    plt.imshow(image[0, 0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"/hdd_mnt/onurcan/onurk/notebooks/example_test_result_large_small/{save_index}_{save_index_monte_carlo}_{save_description}_{save_psnr:.3f}_{save_ssim:.3f}_{save_snr:.3f}.pdf\", bbox_inches='tight', dpi=300, pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                         | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236/236 [14:44<00:00,  3.75s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236/236 [14:44<00:00,  3.75s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236/236 [14:44<00:00,  3.75s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236/236 [14:46<00:00,  3.76s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236/236 [14:45<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, filename=f\"{filename}.log\", filemode=\"w\", format='%(asctime)s %(levelname)-8s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "end2end_model.eval()\n",
    "adversarial_denoiser.eval()\n",
    "with torch.no_grad():\n",
    "    # epoch_mean_metrics = []\n",
    "    all_metrics = []\n",
    "\n",
    "    for i_montecarlo in range(5):\n",
    "        epoch_metrics = {\n",
    "            # \"mse_hio\": np.array([]),\n",
    "            # \"mse_initialization\": np.array([]),\n",
    "            # \"mse_developed\": np.array([]),\n",
    "            \n",
    "            \"snr_dataset\": np.array([]),\n",
    "            \n",
    "            \"psnr_hio\": np.array([]),\n",
    "            \"psnr_initialization\": np.array([]),\n",
    "            \"psnr_developed\": np.array([]),\n",
    "            \"psnr_developed_large\": np.array([]),\n",
    "            \"psnr_developed_large_tta_0\": np.array([]),\n",
    "            \"psnr_developed_large_tta_1\": np.array([]),\n",
    "            \"psnr_developed_large_tta_mean\": np.array([]),\n",
    "            \"psnr_initialization_small\": np.array([]),\n",
    "            \"psnr_developed_small\": np.array([]),\n",
    "            \n",
    "            \"ssim_hio\": np.array([]),\n",
    "            \"ssim_initialization\": np.array([]),\n",
    "            \"ssim_developed\": np.array([]),\n",
    "            \"ssim_developed_large\": np.array([]),\n",
    "            \"ssim_developed_large_tta_0\": np.array([]),\n",
    "            \"ssim_developed_large_tta_1\": np.array([]),\n",
    "            \"ssim_developed_large_tta_mean\": np.array([]),\n",
    "            \"ssim_initialization_small\": np.array([]),\n",
    "            \"ssim_developed_small\": np.array([]),\n",
    "            \n",
    "            \"time_hio\": np.array([]),\n",
    "            \"time_initialization\": np.array([]),\n",
    "            \"time_developed\": np.array([]),\n",
    "            \"time_developed_large\": np.array([]),\n",
    "            \"time_developed_tta\": np.array([]),\n",
    "            \"time_initialization_small\": np.array([]),\n",
    "            \"time_developed_small\": np.array([]),\n",
    "        }\n",
    "\n",
    "        test_i = 0\n",
    "        #for im, am, sp, robust_output in tqdm(test_dataloader):\n",
    "        for im, am, sp, am_noiseless in tqdm(test_dataloader):\n",
    "            # if test_i < 144:\n",
    "            #     test_i += 1\n",
    "            #     continue\n",
    "            im = im.to(device)\n",
    "            am = am.to(device)\n",
    "            sp = sp.to(device)\n",
    "            # robust_output = robust_output.to(device).float()\n",
    "            \n",
    "            # snr calculation\n",
    "            snr = 10*torch.log10(torch.norm(am_noiseless ** 2) / torch.norm(am.cpu() ** 2 - am_noiseless ** 2)).item()\n",
    "            epoch_metrics[\"snr_dataset\"] = np.append(epoch_metrics[\"snr_dataset\"], snr)\n",
    "            \n",
    "            #############\n",
    "            \n",
    "            # HIO method\n",
    "            start_hio = time.time()\n",
    "            \n",
    "            g, _ = get_algorithm(\"HIO\")(am, sp, use_tqdm=False, iteration=1000)\n",
    "            im_cropped_normalized = normalize(crop_center_half(im)) # normalize decreases the metrics\n",
    "            g_cropped_normalized = normalize(crop_center_half(g))\n",
    "            all_mse_loss_corrected = flip_to_minimize_loss(g_cropped_normalized, im_cropped_normalized)\n",
    "            hio_output = all_mse_loss_corrected[0] * 255\n",
    "            \n",
    "            end_hio = time.time()\n",
    "            \n",
    "            #############\n",
    "            \n",
    "            # Developed algorithm - prNet - Large\n",
    "            start_developed = time.time()\n",
    "            \n",
    "            top_k = 10\n",
    "            g, _ = get_algorithm(\"MultiOutputRobustHIO\")(am, sp, different_random_count=100, top_k=top_k, use_tqdm=False)\n",
    "            end_time = time.time()\n",
    "            # print(f\"Time: {end_time - start_time}\")\n",
    "\n",
    "            im_cropped_normalized = crop_center_half(im) # normalize decreases the metrics\n",
    "            g_cropped_normalized = crop_center_half(g)\n",
    "            \n",
    "            im_cropped_normalized_repeated = repeat(\n",
    "                im_cropped_normalized, \"b c h w -> b (repeat c) h w\", repeat=top_k\n",
    "            )\n",
    "\n",
    "            all_mse_loss_corrected = flip_to_minimize_loss(g_cropped_normalized, im_cropped_normalized_repeated)\n",
    "            robust_output = all_mse_loss_corrected[0] # * 255\n",
    "            robust_output = robust_output.to(device).float()\n",
    "            \n",
    "            # clip between 0-255\n",
    "            robust_output = torch.clip(robust_output, 0, 255)\n",
    "\n",
    "            # g, _ = get_algorithm(\"RobustHIO\")(am, sp, use_tqdm=False)\n",
    "            # im_cropped_normalized = normalize(crop_center_half(im)) # normalize decreases the metrics\n",
    "            # g_cropped_normalized = normalize(crop_center_half(g))\n",
    "            # all_mse_loss_corrected = flip_to_minimize_loss(g_cropped_normalized, im_cropped_normalized)\n",
    "            # robust_output = all_mse_loss_corrected[0] * 255\n",
    "            # robust_output = robust_output.to(device).float()\n",
    "            \n",
    "            end_initialization = time.time()\n",
    "\n",
    "            outputs = end2end_model(robust_output, am, sp, total_time_steps=total_iterations, last_iterations_to_train=0)\n",
    "            # output = outputs[-1][:, 0:1, :, :]\n",
    "            output_ = outputs[-1].mean(dim=1, keepdim=True)\n",
    "                \n",
    "            end_developed = time.time()\n",
    "            \n",
    "            output_large = adversarial_denoiser(outputs[-1] / 255.0)[0] * 255.0\n",
    "            \n",
    "            end_developed_large = time.time()\n",
    "            \n",
    "            # TTA - only works for batch size 1\n",
    "            robust_output_augment = repeat(\n",
    "                robust_output, \"b c h w -> (repeat b) c h w\", repeat=2\n",
    "            ).clone()\n",
    "            robust_output_augment[1:2, :, :, :] = robust_output_augment[1:2, :, :, :].flip((-2,-1))\n",
    "            am_tta = repeat(am, \"b c h w -> (repeat b) c h w\", repeat=2).clone()\n",
    "            sp_tta = repeat(sp, \"b c h w -> (repeat b) c h w\", repeat=2).clone()\n",
    "\n",
    "            outputs_tta = end2end_model(robust_output_augment, am_tta, sp_tta, total_time_steps=total_iterations, last_iterations_to_train=0)\n",
    "            output_tta_ = outputs_tta[-1].mean(dim=1, keepdim=True)\n",
    "            output_large_tta = adversarial_denoiser(outputs_tta[-1] / 255.0)[0] * 255.0\n",
    "\n",
    "            output_large_tta[1] = output_large_tta[1].flip((-2,-1))\n",
    "            output_large_tta_mean = reduce(output_large_tta, \"(b 2) c h w -> b c h w\", reduction=\"mean\")\n",
    "\n",
    "            end_developed_tta = time.time()\n",
    "            \n",
    "            #############\n",
    "            \n",
    "            # Developed algorithm - prNet - Small\n",
    "            start_developed_small = time.time()\n",
    "            \n",
    "            g, _ = get_algorithm(\"RobustHIO\")(am, sp, use_tqdm=False)\n",
    "            \n",
    "            end_initialization_small = time.time()\n",
    "            \n",
    "            im_cropped_normalized_small = normalize(crop_center_half(im)) # normalize decreases the metrics\n",
    "            g_cropped_normalized_small = normalize(crop_center_half(g))\n",
    "            all_mse_loss_corrected_small = flip_to_minimize_loss(g_cropped_normalized_small, im_cropped_normalized_small)\n",
    "            robust_output_small = all_mse_loss_corrected_small[0] * 255\n",
    "            robust_output_small = robust_output_small.to(device).float()\n",
    "\n",
    "            outputs_small = end2end_model_small(robust_output_small, am, sp, total_time_steps=total_iterations, last_iterations_to_train=0)\n",
    "            output_small = outputs_small[-1]\n",
    "\n",
    "            end_developed_small = time.time()\n",
    "            \n",
    "            #############\n",
    "\n",
    "            \n",
    "            # measure time\n",
    "            epoch_metrics[\"time_hio\"] = np.append(epoch_metrics[\"time_hio\"], end_hio - start_hio)\n",
    "            epoch_metrics[\"time_initialization\"] = np.append(epoch_metrics[\"time_initialization\"], end_initialization - start_developed)\n",
    "            epoch_metrics[\"time_developed\"] = np.append(epoch_metrics[\"time_developed\"], end_developed - start_developed)\n",
    "            epoch_metrics[\"time_developed_large\"] = np.append(epoch_metrics[\"time_developed_large\"], end_developed_large - start_developed)\n",
    "            epoch_metrics[\"time_developed_tta\"] = np.append(epoch_metrics[\"time_developed_tta\"], end_developed_tta - end_developed_large + end_initialization - start_developed)\n",
    "            epoch_metrics[\"time_initialization_small\"] = np.append(epoch_metrics[\"time_initialization_small\"], end_initialization_small - start_developed_small)\n",
    "            epoch_metrics[\"time_developed_small\"] = np.append(epoch_metrics[\"time_developed_small\"], end_developed_small - start_developed_small)\n",
    "            \n",
    "            # loss calculation\n",
    "            target_im = crop_center_half(im).float()\n",
    "            robust_output_ = robust_output[:, 0:1, :, :]\n",
    "                            \n",
    "            # normalization - makes the metrics worse!\n",
    "            # target_im = normalize(target_im) * 255\n",
    "            # hio_output = normalize(hio_output) * 255\n",
    "            # robust_output = normalize(robust_output) * 255\n",
    "            # outputs[-1] = normalize(outputs[-1]) * 255\n",
    "            \n",
    "            # mse_hio = loss(hio_output, target_im).item()\n",
    "            # epoch_metrics[\"mse_hio\"] = np.append(epoch_metrics[\"mse_hio\"], mse_hio)\n",
    "            # mse_initialization = loss(robust_output, target_im).item()\n",
    "            # epoch_metrics[\"mse_initialization\"] = np.append(epoch_metrics[\"mse_initialization\"], mse_initialization)\n",
    "            # mse_developed = loss(outputs[-1], target_im).item()\n",
    "            # epoch_metrics[\"mse_developed\"] = np.append(epoch_metrics[\"mse_developed\"], mse_developed)\n",
    "            \n",
    "            psnr_hio = peak_signal_noise_ratio(hio_output, target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_hio\"] = np.append(epoch_metrics[\"psnr_hio\"], psnr_hio)\n",
    "            psnr_initialization = peak_signal_noise_ratio(robust_output_, target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_initialization\"] = np.append(epoch_metrics[\"psnr_initialization\"], psnr_initialization)\n",
    "\n",
    "            ssim_hio = structural_similarity_index_measure(hio_output, target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_hio\"] = np.append(epoch_metrics[\"ssim_hio\"], ssim_hio)\n",
    "            ssim_initialization = structural_similarity_index_measure(robust_output_, target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_initialization\"] = np.append(epoch_metrics[\"ssim_initialization\"], ssim_initialization)\n",
    "            \n",
    "            psnr_developed = peak_signal_noise_ratio(output_, target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_developed\"] = np.append(epoch_metrics[\"psnr_developed\"], psnr_developed)\n",
    "            ssim_developed = structural_similarity_index_measure(output_, target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_developed\"] = np.append(epoch_metrics[\"ssim_developed\"], ssim_developed)\n",
    "\n",
    "            psnr_developed_large = peak_signal_noise_ratio(output_large, target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_developed_large\"] = np.append(epoch_metrics[\"psnr_developed_large\"], psnr_developed_large)\n",
    "            ssim_developed_large = structural_similarity_index_measure(output_large, target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_developed_large\"] = np.append(epoch_metrics[\"ssim_developed_large\"], ssim_developed_large)\n",
    "            \n",
    "            psnr_developed_large_tta_0 = peak_signal_noise_ratio(output_large_tta[0:1], target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_developed_large_tta_0\"] = np.append(epoch_metrics[\"psnr_developed_large_tta_0\"], psnr_developed_large_tta_0)\n",
    "            ssim_developed_large_tta_0 = structural_similarity_index_measure(output_large_tta[0:1], target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_developed_large_tta_0\"] = np.append(epoch_metrics[\"ssim_developed_large_tta_0\"], ssim_developed_large_tta_0)\n",
    "            \n",
    "            psnr_developed_large_tta_1 = peak_signal_noise_ratio(output_large_tta[1:2], target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_developed_large_tta_1\"] = np.append(epoch_metrics[\"psnr_developed_large_tta_1\"], psnr_developed_large_tta_1)\n",
    "            ssim_developed_large_tta_1 = structural_similarity_index_measure(output_large_tta[1:2], target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_developed_large_tta_1\"] = np.append(epoch_metrics[\"ssim_developed_large_tta_1\"], ssim_developed_large_tta_1)\n",
    "            \n",
    "            psnr_developed_large_tta_mean = peak_signal_noise_ratio(output_large_tta_mean, target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_developed_large_tta_mean\"] = np.append(epoch_metrics[\"psnr_developed_large_tta_mean\"], psnr_developed_large_tta_mean)\n",
    "            ssim_developed_large_tta_mean = structural_similarity_index_measure(output_large_tta_mean, target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_developed_large_tta_mean\"] = np.append(epoch_metrics[\"ssim_developed_large_tta_mean\"], ssim_developed_large_tta_mean)\n",
    "            \n",
    "            psnr_initialization_small = peak_signal_noise_ratio(robust_output_small, target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_initialization_small\"] = np.append(epoch_metrics[\"psnr_initialization_small\"], psnr_initialization_small)\n",
    "            ssim_initialization_small = structural_similarity_index_measure(robust_output_small, target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_initialization_small\"] = np.append(epoch_metrics[\"ssim_initialization_small\"], ssim_initialization_small)\n",
    "            \n",
    "            psnr_developed_small = peak_signal_noise_ratio(output_small, target_im, data_range=255, reduction=\"none\", dim=(1,2,3)).cpu()\n",
    "            epoch_metrics[\"psnr_developed_small\"] = np.append(epoch_metrics[\"psnr_developed_small\"], psnr_developed_small)\n",
    "            ssim_developed_small = structural_similarity_index_measure(output_small, target_im, data_range=255, reduction=\"none\").cpu()\n",
    "            epoch_metrics[\"ssim_developed_small\"] = np.append(epoch_metrics[\"ssim_developed_small\"], ssim_developed_small)\n",
    "            \n",
    "            # save figures\n",
    "            # save_fig(target_im, test_i, i_montecarlo, 0.0, 0.0, snr, \"gt\")\n",
    "            # save_fig(hio_output, test_i, i_montecarlo, psnr_hio.mean(), ssim_hio.mean(), snr, \"hio\")\n",
    "            # save_fig(robust_output_, test_i, i_montecarlo, psnr_initialization.mean(), ssim_initialization.mean(), snr, \"large_initialization\")\n",
    "            # save_fig(output_, test_i, i_montecarlo, psnr_developed.mean(), ssim_developed.mean(), snr, \"large_main_loop\")\n",
    "            # save_fig(output_large, test_i, i_montecarlo, psnr_developed_large.mean(), ssim_developed_large.mean(), snr, \"large_main_loop_adversarial\")\n",
    "            # save_fig(output_large_tta_mean, test_i, i_montecarlo, psnr_developed_large_tta_mean.mean(), ssim_developed_large_tta_mean.mean(), snr, \"large_main_loop_adversarial_tta\")\n",
    "            # save_fig(robust_output_small, test_i, i_montecarlo, psnr_initialization_small.mean(), ssim_initialization_small.mean(), snr, \"small_initialization\")\n",
    "            # save_fig(output_small, test_i, i_montecarlo, psnr_developed_small.mean(), ssim_developed_small.mean(), snr, \"small_main_loop\")\n",
    "            \n",
    "            test_i += 1\n",
    "        \n",
    "        epoch_metrics_mean = {k: v.mean() for k, v in epoch_metrics.items()}\n",
    "        all_metrics.append(epoch_metrics)\n",
    "        # epoch_mean_metrics.append(epoch_metrics_mean)\n",
    "        logging.info(epoch_metrics_mean)\n",
    "\n",
    "    logging.info(\"Evaluation finished\")\n",
    "    \n",
    "    # save all_metrics\n",
    "    with open(f\"{filename}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_metrics, f)\n",
    "    \n",
    "    # epoch_mean_metrics = {k: np.array([v[k] for v in epoch_mean_metrics]).mean() for k in epoch_mean_metrics[0].keys()}\n",
    "    # logging.info(epoch_mean_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle\n",
    "all_metrics = pickle.load(open(f\"{filename}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_indices = np.arange(len(all_metrics[0]['psnr_hio']))\n",
    "unnatural_images_slice = np.arange(114, 120)\n",
    "natural_images_slice = np.setdiff1d(all_image_indices, unnatural_images_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall \t | \t Natural \t | \t Unnatural \t |\n",
      "30.24 \t \t | \t 30.31 \t \t | \t 27.22 \t \t | \t snr_dataset\n",
      "18.52 \t \t | \t 18.48 \t \t | \t 19.80 \t \t | \t psnr_hio\n",
      "21.14 \t \t | \t 21.14 \t \t | \t 21.52 \t \t | \t psnr_initialization\n",
      "28.29 \t \t | \t 28.35 \t \t | \t 25.98 \t \t | \t psnr_developed\n",
      "28.29 \t \t | \t 28.35 \t \t | \t 25.91 \t \t | \t psnr_developed_large\n",
      "28.28 \t \t | \t 28.35 \t \t | \t 25.89 \t \t | \t psnr_developed_large_tta_0\n",
      "28.27 \t \t | \t 28.32 \t \t | \t 26.45 \t \t | \t psnr_developed_large_tta_1\n",
      "28.56 \t \t | \t 28.61 \t \t | \t 26.49 \t \t | \t psnr_developed_large_tta_mean\n",
      "19.12 \t \t | \t 19.08 \t \t | \t 20.63 \t \t | \t psnr_initialization_small\n",
      "26.69 \t \t | \t 26.75 \t \t | \t 24.48 \t \t | \t psnr_developed_small\n",
      "0.39 \t \t | \t 0.39 \t \t | \t 0.40 \t \t | \t ssim_hio\n",
      "0.41 \t \t | \t 0.41 \t \t | \t 0.43 \t \t | \t ssim_initialization\n",
      "0.81 \t \t | \t 0.81 \t \t | \t 0.72 \t \t | \t ssim_developed\n",
      "0.81 \t \t | \t 0.81 \t \t | \t 0.72 \t \t | \t ssim_developed_large\n",
      "0.81 \t \t | \t 0.81 \t \t | \t 0.72 \t \t | \t ssim_developed_large_tta_0\n",
      "0.81 \t \t | \t 0.81 \t \t | \t 0.72 \t \t | \t ssim_developed_large_tta_1\n",
      "0.82 \t \t | \t 0.82 \t \t | \t 0.73 \t \t | \t ssim_developed_large_tta_mean\n",
      "0.41 \t \t | \t 0.41 \t \t | \t 0.41 \t \t | \t ssim_initialization_small\n",
      "0.76 \t \t | \t 0.76 \t \t | \t 0.67 \t \t | \t ssim_developed_small\n",
      "0.28 \t \t | \t 0.28 \t \t | \t 0.28 \t \t | \t time_hio\n",
      "0.91 \t \t | \t 0.91 \t \t | \t 0.91 \t \t | \t time_initialization\n",
      "1.48 \t \t | \t 1.48 \t \t | \t 1.48 \t \t | \t time_developed\n",
      "1.52 \t \t | \t 1.52 \t \t | \t 1.52 \t \t | \t time_developed_large\n",
      "1.81 \t \t | \t 1.81 \t \t | \t 1.81 \t \t | \t time_developed_tta\n",
      "0.48 \t \t | \t 0.48 \t \t | \t 0.48 \t \t | \t time_initialization_small\n",
      "1.02 \t \t | \t 1.02 \t \t | \t 1.02 \t \t | \t time_developed_small\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall \\t | \\t Natural \\t | \\t Unnatural \\t |\")\n",
    "for metric in all_metrics[0].keys():\n",
    "    overall, natural, unnatural = np.array([v[metric].mean() for v in all_metrics]).mean(), np.array([v[metric][natural_images_slice].mean() for v in all_metrics]).mean(), np.array([v[metric][unnatural_images_slice].mean() for v in all_metrics]).mean()\n",
    "    print(f\"{overall:.2f} \\t \\t | \\t {natural:.2f} \\t \\t | \\t {unnatural:.2f} \\t \\t | \\t {metric}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
